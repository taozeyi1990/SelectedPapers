# Paper Hightlights
# ICML 2018
## Orals
- [Transfer Learning via Learning to Transfer](http://proceedings.mlr.press/v80/wei18a.html)
- [Asynchronous Decentralized Parallel Stochastic Gradient Descent](http://proceedings.mlr.press/v80/lian18a.html)
- [D2:Decentralized Training over Decentralized Data](http://proceedings.mlr.press/v80/tang18a.html)
- [Stochastic Variance-Reduced Cubic Regularized Newton Methods](http://proceedings.mlr.press/v80/zhou18d.html)
- [Communication-Computation Efficient Gradient Coding](http://proceedings.mlr.press/v80/ye18a.html)
- [Fast Variance Reduction Method with Stochastic Batch Size](http://proceedings.mlr.press/v80/liu18f.html)
- [A Simple Stochastic Variance Reduced Algorithm with Fast Convergence Rates](http://proceedings.mlr.press/v80/zhou18c.html)
- [Compressing Neural Networks using the Variational Information Bottleneck](http://proceedings.mlr.press/v80/dai18d.html)
- [Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization](http://proceedings.mlr.press/v80/simsekli18a.html)
- [Improved Training of Generative Adversarial Networks Using Representative Features](http://proceedings.mlr.press/v80/bang18a.html)
- [Knowledge Transfer with Jacobian Matching](http://proceedings.mlr.press/v80/srinivas18a.html)
- [Differentially Private Identity and Equivalence Testing of Discrete Distributions](http://proceedings.mlr.press/v80/aliakbarpour18a.html)

## Parallel and Distributed Learning Seesion
- [The Hidden Vulnerability of Distributed Learning in Byzantium](http://proceedings.mlr.press/v80/mhamdi18a.html)
- [Asynchronous Byzantine Machine Learning (the case of SGD)](http://proceedings.mlr.press/v80/damaskinos18a.html)
- [DRACO: Byzantine-resilient Distributed Training via Redundant Gradients](http://proceedings.mlr.press/v80/chen18l.html)

# ICLR 2016
## Accept Papers
- [Neural Architecture Search with Reinforcement Learning](https://openreview.net/pdf?id=r1Ue8Hcxg)
> Need More reading
- [Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://openreview.net/pdf?id=HkwoSDPgg)
> Need More reading
- [Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer](https://openreview.net/pdf?id=Sks9_ajex)
> Famous Attention Transfer
- [Learning to Optimize](https://openreview.net/pdf?id=ry4Vrt5gl)
- [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://openreview.net/pdf?id=SJGCiw5gl)
- [A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks](https://openreview.net/pdf?id=Hkg4TI9xl)
> Recent read list
- [Online Bayesian Transfer Learning for Sequential Data Modeling](https://openreview.net/pdf?id=ByqiJIqxg)
- [Paleo: A Performance Model for Deep Neural Networks](https://openreview.net/pdf?id=SyVVJ85lg)
- [SGDR: Stochastic Gradient Descent with Warm Restarts](https://openreview.net/pdf?id=Skq89Scxx)
- [Entropy-SGD: Biasing Gradient Descent Into Wide Valleys](https://openreview.net/pdf?id=B1YfAfcgl)
- [Do Deep Convolutional Nets Really Need to be Deep and Convolutional?](https://openreview.net/pdf?id=r10FA8Kxg)
- [Why Deep Neural Networks for Function Approximation?](https://openreview.net/forum?id=SkpSlKIel)
## WorkShops
# ICLR 2017
